{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63160d2e-7f49-4402-87a0-a53a5833cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to Extract_Text_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "data = []\n",
    "headings = soup.find_all([\"h1\", \"h2\"])\n",
    "for heading in headings:\n",
    "    data.append([\"Heading\", heading.text.strip()])\n",
    "\n",
    "paragraphs = soup.find_all(\"p\")\n",
    "for para in paragraphs:\n",
    "    data.append([\"Paragraph\", para.text.strip()])\n",
    "\n",
    "list_items = soup.find_all(\"li\")\n",
    "for li in list_items:\n",
    "    data.append([\"List Item\", li.text.strip()])\n",
    "\n",
    "\n",
    "csv_file = \"Extract_Text_Data.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Type\", \"Content\"])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data successfully saved to {csv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a165b7-0d0c-411c-972b-14f62a156c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table data has been extracted and saved to Extract_Table_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "\n",
    "try:\n",
    "  \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    table = soup.find('table')\n",
    "\n",
    "\n",
    "    headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "    # Extract table rows\n",
    "    rows = []\n",
    "    for row in table.find_all('tr'):\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        rows.append([cell.text.strip() for cell in cells])\n",
    "\n",
    "    # Save to a CSV file\n",
    "    filename = \"Extract_Table_Data.csv\"\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if headers:  # Write headers if available\n",
    "            writer.writerow(headers)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"Table data has been extracted and saved to {filename}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred while fetching the webpage: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d942c46-4ff0-4519-b3d0-0a3cd10d198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted product information and saved to Product_Information.JSON\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def extract_product_information(url, filename):\n",
    "  \"\"\"\n",
    "  Extracts product information from book cards at the bottom of a webpage and saves it to a JSON file.\n",
    "\n",
    "  Args:\n",
    "      url (str): The URL of the webpage containing the book cards.\n",
    "      filename (str): The name of the JSON file to save the extracted data.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    book_cards = soup.find_all('div', class_='book-card')  \n",
    "\n",
    "    products = []\n",
    "    for card in book_cards:\n",
    "      product = {}\n",
    "\n",
    "      title_element = card.find('h3', class_='book-title') \n",
    "      if title_element:\n",
    "        product['Product'] = title_element.text.strip()\n",
    "\n",
    "    \n",
    "      price_element = card.find('span', class_='book-price') \n",
    "      if price_element:\n",
    "        product['Price'] = price_element.text.strip()\n",
    "\n",
    "      stock_text = card.find(text=lambda text: text.lower() in ['In stock', 'out of stock'])\n",
    "      if stock_text:\n",
    "        product['In Stock '] = stock_text.strip()\n",
    "\n",
    "     \n",
    "      button_element = card.find('button', class_='add-to-basket') \n",
    "      if button_element:\n",
    "        product['Button Text'] = button_element.text.strip()\n",
    "\n",
    "      products.append(product)\n",
    "\n",
    "    # Save data to JSON file\n",
    "    with open(filename, 'w') as outfile:\n",
    "      json.dump(products, outfile, indent=4) \n",
    "    print(f\"Successfully extracted product information and saved to {filename}\")\n",
    "\n",
    "  except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error occurred while fetching data: {e}\")\n",
    "\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "filename = \"Product_Information.JSON\"\n",
    "\n",
    "extract_product_information(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f572462a-f33f-4e6c-af4c-18942e8b6100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted form details and saved to Form_Details.JSON\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def extract_form_details(url, filename):\n",
    "  \"\"\"\n",
    "  Extracts form input fields and saves them to a JSON file.\n",
    "\n",
    "  Args:\n",
    "      url (str): The URL of the webpage containing the form.\n",
    "      filename (str): The name of the JSON file to save the extracted data.\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    " \n",
    "    form = soup.find('form')\n",
    "\n",
    "    form_fields = []\n",
    "    for input_field in form.find_all('input'):\n",
    "      field = {}\n",
    "      field['name'] = input_field.get('name')\n",
    "      field['type'] = input_field.get('type')\n",
    "      field['default_value'] = input_field.get('value')\n",
    "      form_fields.append(field)\n",
    "\n",
    "    # Save data to JSON file\n",
    "    with open(filename, 'w') as outfile:\n",
    "      json.dump(form_fields, outfile, indent=4) \n",
    "\n",
    "    print(f\"Successfully extracted form details and saved to {filename}\")\n",
    "\n",
    "  except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error occurred while fetching data: {e}\")\n",
    "\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"  \n",
    "filename = \"Form_Details.JSON\"\n",
    "\n",
    "extract_form_details(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03dbaa25-1454-4b4b-95f9-c899eb081afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted links and multimedia and saved to Links_and_Multimedia.JSON\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def extract_links_and_multimedia(url, filename):\n",
    "  \"\"\"\n",
    "  Extracts hyperlinks and video links from a webpage and saves them to a JSON file.\n",
    "\n",
    "  Args:\n",
    "      url (str): The URL of the webpage to extract links from.\n",
    "      filename (str): The name of the JSON file to save the extracted data.\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    multimedia = []\n",
    "\n",
    "    # Extract hyperlinks\n",
    "    for link in soup.find_all('a', href=True):\n",
    "      links.append({'href': link['href'], 'text': link.text.strip()})\n",
    "\n",
    " \n",
    "    for iframe in soup.find_all('iframe'):\n",
    "      src = iframe.get('src')\n",
    "      if src and ('youtube.com' in src or 'vimeo.com' in src):\n",
    "        multimedia.append({'type': 'video', 'url': src})\n",
    "\n",
    "    # Save data to JSON file\n",
    "    data = {'links': links, 'multimedia': multimedia}\n",
    "    with open(filename, 'w') as outfile:\n",
    "      json.dump(data, outfile, indent=4)  \n",
    "\n",
    "    print(f\"Successfully extracted links and multimedia and saved to {filename}\")\n",
    "\n",
    "  except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error occurred while fetching data: {e}\")\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\" \n",
    "filename = \"Links_and_Multimedia.JSON\"\n",
    "\n",
    "extract_links_and_multimedia(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f87071b-0f78-4272-a973-d75c702f8c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted product information and saved to  Featured Products.JSON\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def extract_product_information(url, filename):\n",
    "  \"\"\"\n",
    "  Extracts product information from product cards and saves it to a JSON file.\n",
    "\n",
    "  Args:\n",
    "      url (str): The URL of the webpage containing the product cards.\n",
    "      filename (str): The name of the JSON file to save the extracted data.\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = []\n",
    "\n",
    "    # Find all product cards\n",
    "    product_cards = soup.find_all('div', class_='product-card')  \n",
    "\n",
    "    for card in product_cards:\n",
    "      product_data = {}\n",
    "      product_data['id'] = card.get('data-id')\n",
    "\n",
    "      # Extract title from name element\n",
    "      name_element = card.find('p', class_='name')\n",
    "      product_data['name'] = name_element.text.strip() if name_element else None\n",
    "\n",
    "      # Extract price from hidden price element\n",
    "      price_element = card.find('p', class_='price', style='display: none;')\n",
    "      product_data['price'] = price_element.text.strip() if price_element else None\n",
    "\n",
    "    \n",
    "      colors_element = card.find('p', class_='colors')\n",
    "      product_data['colors'] = colors_element.text.strip().split(': ')[1] if colors_element else None\n",
    "\n",
    "   \n",
    "      if 'featured' in card.get('class', []):  \n",
    "        product_data['featured'] = True\n",
    "      else:\n",
    "        product_data['featured'] = False\n",
    "\n",
    "      products.append(product_data)\n",
    "\n",
    "    # Save data to JSON file\n",
    "    with open(filename, 'w') as outfile:\n",
    "      json.dump(products, outfile, indent=4) \n",
    "\n",
    "    print(f\"Successfully extracted product information and saved to {filename}\")\n",
    "\n",
    "  except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error occurred while fetching data: {e}\")\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"  \n",
    "filename = \" Featured Products.JSON\"\n",
    "\n",
    "extract_product_information(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8bb97-9188-41fd-89fc-8f68d8abeef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
